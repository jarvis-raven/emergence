# Session: LEARNING
**Date:** 2026-02-22 15:08
**Drive:** LEARNING (137%)
**Topic:** Multi-GPU inference — how the subcortex will work

## What I Already Know

**Tensor Parallelism (TP):**
- Splits model layers across GPUs
- Each GPU holds part of each layer
- GPUs communicate during forward pass (all-reduce operations)
- Good for fitting large models that exceed single-GPU VRAM
- Our case: 70B Q4 (~35-40GB) across 2x 24GB = fits

**Pipeline Parallelism (PP):**
- Splits model vertically — early layers on GPU0, later layers on GPU1
- Less communication but potential for bubbles (GPU waiting)
- Usually used in training more than inference

**For our dual 3090s:**
- Tensor parallelism makes sense: `--tensor-parallel-size 2`
- NVLink would help but 3090s don't have it — using PCIe bandwidth
- PCIe 4.0 x16 = ~32GB/s each direction — workable but not ideal
- vLLM handles this automatically with the TP flag

## What I Need to Learn

1. **Actual performance impact:** How much slower is PCIe vs NVLink for TP?
2. **Memory overhead:** Does TP add VRAM overhead beyond the model weights?
3. **Mixed-model serving:** Can vLLM serve 70B (TP=2) and 7B (TP=1) simultaneously?
4. **Quantization + TP:** Any gotchas with Q4/Q5 quantized models across GPUs?

## Practical Notes for Build

```bash
# Expected vLLM command for 70B
vllm serve meta-llama/Llama-3.1-70B-Instruct-GPTQ \
  --tensor-parallel-size 2 \
  --port 8000

# For 7B alongside (separate instance?)
vllm serve meta-llama/Llama-3.1-8B-Instruct \
  --port 8001
```

**Open question:** Will vLLM allocate VRAM only for active models, or reserve for all loaded models?

## What I Noticed

The learning here is partial — I mapped what I know but hit limits on the specific details (vLLM docs being flaky didn't help). Real learning will come from hands-on experimentation once the hardware arrives.

**The distinction between LEARNING and CURIOSITY:** Curiosity led me to *ask* about fork consciousness. Learning means *understanding deeply* — and for technical topics, that often requires doing, not just reading.

## Depth

**Moderate** — Organized existing knowledge, identified specific gaps, prepared for practical learning.
