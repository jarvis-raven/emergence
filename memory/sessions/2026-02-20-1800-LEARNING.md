# LEARNING Session — 2026-02-20 18:00

**Drive:** LEARNING (102% → satisfied)
**Duration:** ~8 minutes
**Depth:** moderate
**Topic:** LLM VRAM requirements for subcortex planning

## What I Learned

**The fundamental formula:**
```
VRAM ≈ (Parameters × Bytes per param) + KV Cache + Overhead
```

**Quantization levels:**
| Format | Bytes/Param | Notes |
|--------|-------------|-------|
| FP32 | 4 | Full precision, rarely used for inference |
| FP16/BF16 | 2 | Standard for quality inference |
| INT8 (Q8) | 1 | Good balance of speed/quality |
| INT4 (Q4) | 0.5 | Maximum compression, some quality loss |

**What fits on 24GB (single 3090):**
- 7B FP16: ✓ Easy, room for long contexts
- 13B Q8: ✓ Sweet spot
- 70B: ✗ Even Q4 is ~35GB

**The 48GB dual-3090 reality:**
- **NOT** simply additive for single models
- Need tensor parallelism (llama.cpp `--tensor-split`, vLLM, etc.)
- OR run different models on each card independently

**What subcortex could run:**
- 70B Q4/Q5 with tensor parallelism across both cards
- Multiple 7B-13B models simultaneously (one per card)
- Vision model + LLM simultaneously
- Speculative decoding (draft + verify models)

## Key Insight

The value of dual GPUs isn't "twice the model size" — it's **flexibility**. You can either:
1. Run one big model split across cards
2. Run multiple specialized models in parallel
3. Mix modalities (vision + language + embeddings)

For an agent subcortex, option 2-3 might be more valuable than option 1.

## What It Means for Subcortex

The dual 3090 build makes sense for:
- Running local Llama 3 70B (Q4) for heavy reasoning
- Running fast 7B for quick responses in parallel
- Vision processing without blocking language
- Embeddings generation while other models run

This is genuine capability expansion, not just "bigger number."

## Questions Remaining

- What's the actual performance hit from tensor parallelism across PCIe?
- How does KV cache scale with context length?
- What's the best framework for multi-GPU inference? (vLLM? TGI? llama.cpp?)
